{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4eb3892c198b52d2"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-03T15:56:11.336710900Z",
     "start_time": "2024-03-03T15:56:09.875642800Z"
    }
   },
   "outputs": [],
   "source": [
    "from modeling.backbone.vit_adapter import ViTAdapter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vit_adapter = ViTAdapter(\n",
    "    pretrain_size=518,\n",
    "    num_heads=24,\n",
    "    conv_inplane=64,\n",
    "    patch_size=14,\n",
    "    embed_dim=1536,\n",
    "    depth=40,\n",
    "    mlp_ratio=4,\n",
    "    drop_path_rate=0.4,\n",
    "    n_points=4,\n",
    "    deform_num_heads=24,\n",
    "    cffn_ratio=0.25,\n",
    "    deform_ratio=0.5,\n",
    "    \n",
    "    interaction_indexes=[[0, 9], [10, 19], [20, 29], [30, 39]],\n",
    "    window_attn=[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
    "    window_size=[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
    "    freeze_vit=True,\n",
    "    use_cls=True,\n",
    "    img_size=518,\n",
    "    ffn_type='swiglu'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T15:57:24.092332Z",
     "start_time": "2024-03-03T15:56:12.280015700Z"
    }
   },
   "id": "6be204f84ec655",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Vit Input"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "570f7759306525d0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/e/dinov2-main/data/vit_before_input_x.pth'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m vit_input \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/mnt/e/dinov2-main/data/vit_before_input_x.pth\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m vit_output_1 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/e/dinov2-main/data/vit_after_input_f1.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m vit_output_2 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/e/dinov2-main/data/vit_after_input_f2.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:579\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    577\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 579\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m    581\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m    582\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m    583\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[1;32m    584\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:230\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 230\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:211\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 211\u001B[0m     \u001B[38;5;28msuper\u001B[39m(_open_file, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/mnt/e/dinov2-main/data/vit_before_input_x.pth'"
     ]
    }
   ],
   "source": [
    "vit_input = torch.load(\"/mnt/e/dinov2-main/data/vit_before_input_x.pth\")\n",
    "vit_output_1 = torch.load(\"/mnt/e/dinov2-main/data/vit_after_input_f1.pth\")\n",
    "vit_output_2 = torch.load(\"/mnt/e/dinov2-main/data/vit_after_input_f2.pth\")\n",
    "vit_output_3 = torch.load(\"/mnt/e/dinov2-main/data/vit_after_input_f3.pth\")\n",
    "vit_output_4 = torch.load(\"/mnt/e/dinov2-main/data/vit_after_input_f4.pth\")\n",
    "print(vit_input.size())\n",
    "print(vit_output_1.size())\n",
    "print(vit_output_2.size())\n",
    "print(vit_output_3.size())\n",
    "print(vit_output_4.size())\n",
    "print(vit_output_4.mean())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T15:57:55.139909200Z",
     "start_time": "2024-03-03T15:57:55.050621700Z"
    }
   },
   "id": "ea2da45c6ac20d86",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuokyong/miniconda3/envs/ovseg/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "output = vit_adapter(vit_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T08:16:06.361674Z",
     "start_time": "2024-03-02T08:13:30.704260Z"
    }
   },
   "id": "2068feab6ca906a7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 1536, 128, 128])\n",
      "tensor(-1.1156e-10, grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1536, 64, 64])\n",
      "tensor(9.3617e-10, grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1536, 32, 32])\n",
      "tensor(-2.0858e-09, grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1536, 16, 16])\n",
      "tensor(9.7013e-11, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(len(output))\n",
    "for single_output in output:\n",
    "    print(single_output.size())\n",
    "    print(single_output.mean())\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T08:18:33.561529600Z",
     "start_time": "2024-03-02T08:18:33.375347Z"
    }
   },
   "id": "5cd715e8e962a5eb",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f22cd448db1b5152"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
